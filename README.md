<p align="center">
  <img src="https://github.com/user-attachments/assets/d0b05b81-d168-498c-ae67-87c46e638817" width="800" />
</p>

<h1 align="center">üöÄ Insightra</h1>

<p align="center">
  <b>From Data to Decisions.</b><br>
  AI-powered RAG backend to upload PDFs, scan URLs, ingest raw text, and retrieve intelligent, context-aware insights.
</p>

<p align="center">
  <a href="#"><img src="https://img.shields.io/badge/FastAPI-Backend-success" /></a>
  <a href="#"><img src="https://img.shields.io/badge/Python-3.10+-blue" /></a>
  <a href="#"><img src="https://img.shields.io/badge/LLM-Groq-purple" /></a>
  <a href="#"><img src="https://img.shields.io/badge/Search-Tavily-orange" /></a>
  <a href="#"><img src="https://img.shields.io/badge/Framework-LangChain-red" /></a>
  <a href="#"><img src="https://img.shields.io/badge/Deploy-Render-black" /></a>
</p>

---

## üåü Why Insightra?

Modern information is scattered across documents, web pages, and raw text. **Insightra** unifies all of this into a single intelligent search and insight engine powered by RAG (Retrieval-Augmented Generation) architecture.

With Insightra, you can:
- üìÑ Upload and process documents  
- üåê Crawl and extract web content  
- üß† Build searchable knowledge bases  
- üîç Query using natural language  
- üìö Get AI-powered, source-backed answers  

---

## ‚ú® Core Features

| Feature | Description |
|---------|-------------|
| üìÑ PDF Ingestion | Upload and parse PDF documents |
| üåê URL Crawling | Extract structured content from webpages |
| üß† RAG Architecture | Retrieval-Augmented Generation using LangChain |
| üîç Semantic Search | Vector-based similarity matching |
| üìö Source Citations | Transparent answer sourcing |
| ‚ö° FastAPI Backend | High-performance async APIs |
| üîê Secure Configuration | Environment-based API keys |
| ‚òÅÔ∏è Cloud Ready | Deploy on Render, Railway, or AWS |
| üß© Modular Design | Easy to extend and customize |

---

## üß† How It Works

1. **Ingest** ‚Äì Upload PDFs, URLs, or raw text via API  
2. **Process** ‚Äì Extract, chunk, and clean content using LangChain text splitters  
3. **Embed** ‚Äì Generate vector embeddings for semantic search  
4. **Store** ‚Äì Save in vector database with metadata  
5. **Retrieve** ‚Äì Semantic + keyword search across stored knowledge  
6. **Generate** ‚Äì LLM synthesizes context-aware answers using retrieved chunks  

---

## üèóÔ∏è Tech Stack

| Layer | Technology |
|-------|------------|
| ‚öôÔ∏è Backend | FastAPI |
| üß† LLM | Groq (llama-3.1-70b) |
| üîó Framework | LangChain |
| üåê Web Search | Tavily API |
| üóÑÔ∏è Vector Store | Chroma / FAISS |
| üìä Embeddings | Sentence Transformers |
| üêç Language | Python 3.10+ |
| ‚òÅÔ∏è Deployment | Render |

---

## üöÄ Quick Start

### Prerequisites

```bash
python >= 3.10
pip
virtualenv (recommended)
```

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/insightra.git
cd insightra

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Create a `.env` file in the root directory:

```env
GROQ_API_KEY=your_groq_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
```

### Run the Server

```bash
uvicorn main:app --reload
```

The API will be available at `http://localhost:8000`

---



### 4. Health Check
```bash
GET /health
```

---

## üîê Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `GROQ_API_KEY` | API key for Groq LLM | Yes |
| `TAVILY_API_KEY` | API key for Tavily search | Yes |
| `VECTOR_DB_PATH` | Path to vector database | No (default: ./db) |
| `MAX_FILE_SIZE` | Max upload size in MB | No (default: 10) |

---
